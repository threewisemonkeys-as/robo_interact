import time
import logging
import random
from pathlib import Path
import base64
import mimetypes
import traceback
from joblib import Memory
from typing import List, Dict, Tuple, Optional, Union, Any
import io

memory = Memory('.cache', verbose=0)

from dotenv import load_dotenv
load_dotenv()

import numpy as np
import matplotlib
matplotlib.use('Agg')  # Set this before importing pyplot
import matplotlib.pyplot as plt

import torch
from PIL import Image
from cv_bridge import CvBridge
import pyrealsense2 as rs
import openai

import rospy
from sensor_msgs.msg import Image as ROSSensorImage 
from sensor_msgs.msg import CameraInfo
from visualization_msgs.msg import Marker, MarkerArray
import tf2_geometry_msgs
import tf2_ros
import geometry_msgs.msg

from interbotix_xs_modules.arm import InterbotixManipulatorXS

from cs2 import SAMClient


CUR_DIR = Path(__file__).resolve().parent

# Global configuration flags
USE_VLM = True  # Set to False to use human selection instead
MODEL_NAME = "sam_remote"
IMAGES_DIR = CUR_DIR / "logs"


CAMERA_FRAME = "camera_color_optical_frame"
ROBOT_FRAME = "wx250s/base_link"


# Define custom exception hierarchy
class RoboMindException(Exception):
    """Base exception for RoboMind operations."""
    pass

class ImageCaptureException(RoboMindException):
    """Exception raised when image capture fails."""
    pass

class PointGenerationException(RoboMindException):
    """Exception raised when keypoint generation fails."""
    pass

class Point3DException(RoboMindException):
    """Exception raised when 3D point conversion fails."""
    pass

class RobotOperationException(RoboMindException):
    """Exception raised when robot operation fails."""
    pass


# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S',
    force=True  # Force override any existing configuration
)

# Create console handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
console_handler.setFormatter(formatter)

# Get logger and add handlers
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)
logger.addHandler(console_handler)

# Make sure logs will display
logger.propagate = True

# Test log message
logger.info("Logging system initialized")


def show_anns(anns: List[Dict[str, Any]], 
              borders: bool = True, 
              show_boxes: bool = True, 
              show_points: bool = True, 
              show_masks: bool = True, 
              sampled_points: Optional[Union[List[Tuple[float, float]], np.ndarray]] = None,
              sampled_point_marker: str = 'o', 
              sampled_point_size: int = 30, 
              sampled_point_color: Union[str, Tuple[float, float, float]] = 'green') -> None:
    """
    Visualize segmentation masks and points.
    
    This function visualizes the segmentation masks, bounding boxes, and points
    generated by mask models like SAM. It can also overlay sampled points.
    
    Args:
        anns: List of mask dictionaries, each containing segmentation data
        borders: Whether to show mask borders
        show_boxes: Whether to show bounding boxes
        show_points: Whether to show original points
        show_masks: Whether to color and display the masks
        sampled_points: List or array of (x, y) coordinates for sampled points
        sampled_point_marker: Marker style for sampled points (matplotlib marker style)
        sampled_point_size: Size of sampled points
        sampled_point_color: Color of sampled points (matplotlib color)
    """
    logger.info(f"Starting visualization of {len(anns)} annotations")
    start_time = time.time()
    
    if len(anns) == 0:
        logger.warning("No annotations to visualize")
        return
    
    sorted_anns = sorted(anns, key=(lambda x: x['area']), reverse=True)
    ax = plt.gca()
    ax.set_autoscale_on(False)
    img = np.ones((sorted_anns[0]['segmentation'].shape[0], sorted_anns[0]['segmentation'].shape[1], 4))
    img[:, :, 3] = 0
    
    if show_masks:
        logger.info(f"Processing {len(sorted_anns)} masks for visualization")
        mask_start = time.time()
        for i, ann in enumerate(sorted_anns):
            if i % 50 == 0 and i > 0:  # Log progress every 50 masks
                logger.info(f"Processed {i}/{len(sorted_anns)} masks")
            
            m = ann['segmentation']
            color_mask = np.concatenate([np.random.random(3), [0.5]])
            img[m] = color_mask
            
            # Draw bounding box
            if show_boxes and 'bbox' in ann:
                x, y, w, h = ann['bbox']
                rect = plt.Rectangle((x, y), w, h, linewidth=1.5, 
                                    edgecolor=color_mask[:3], facecolor='none')
                ax.add_patch(rect)
        
        logger.info(f"Mask coloring completed in {time.time() - mask_start:.2f} seconds")
    elif show_boxes:
        # If we're not showing masks but want to show boxes, we still need to draw them
        logger.info("Drawing bounding boxes without masks")
        for i, ann in enumerate(sorted_anns):
            if 'bbox' in ann:
                x, y, w, h = ann['bbox']
                # Use a default color since we don't have mask colors
                rect = plt.Rectangle((x, y), w, h, linewidth=1.5, 
                                    edgecolor='blue', facecolor='none')
                ax.add_patch(rect)
    
    if borders and show_masks:  # Only show borders if masks are enabled
        logger.info("Adding contour borders to visualization")
        border_start = time.time()
        import cv2
        for i, ann in enumerate(sorted_anns):
            if i % 50 == 0 and i > 0:  # Log progress every 50 contours
                logger.info(f"Processed {i}/{len(sorted_anns)} contours")
                
            m = ann['segmentation']
            contours, _ = cv2.findContours(m.astype(np.uint8), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)
            # Try to smooth contours
            contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]
            cv2.drawContours(img, contours, -1, (0, 0, 1, 0.4), thickness=1)
        logger.info(f"Border drawing completed in {time.time() - border_start:.2f} seconds")
    
    # Draw original point coordinates
    if show_points:
        points_start = time.time()
        logger.info("Adding original point coordinates to visualization")
        for i, ann in enumerate(sorted_anns):
            if 'point_coords' in ann and ann['point_coords'] is not None:
                points = ann['point_coords']
                # Handle points as list or numpy array
                if isinstance(points, list) and len(points) > 0:
                    # Convert list to numpy array if needed
                    points = np.array(points)
                    # Use mask color for points if masks are shown, otherwise use a default color
                    if show_masks:
                        x, y = int(points[0][0]), int(points[0][1])
                        if 0 <= y < img.shape[0] and 0 <= x < img.shape[1]:
                            point_color = img[y, x, :3]
                        else:
                            point_color = np.concatenate([np.random.random(3), [0.5]])[:3]  # Fallback color
                    else:
                        point_color = 'red'  # Default color when masks are not shown
                        
                    ax.scatter(points[:, 0], points[:, 1], color=point_color,
                              s=20, marker='*', edgecolors='white')
                elif hasattr(points, 'size') and points.size > 0:
                    # Already a numpy array with points
                    if show_masks:
                        x, y = int(points[0][0]), int(points[0][1])
                        if 0 <= y < img.shape[0] and 0 <= x < img.shape[1]:
                            point_color = img[y, x, :3]
                        else:
                            point_color = np.concatenate([np.random.random(3), [0.5]])[:3]  # Fallback color
                    else:
                        point_color = 'red'  # Default color when masks are not shown
                        
                    ax.scatter(points[:, 0], points[:, 1], color=point_color,
                              s=20, marker='*', edgecolors='white')
        logger.info(f"Original point visualization completed in {time.time() - points_start:.2f} seconds")
    
    # Draw sampled points
    if sampled_points is not None:
        sampled_points_start = time.time()
        logger.info("Adding sampled points to visualization")
        for i, ann in enumerate(sorted_anns):
            points = sampled_points
            # Handle points as list or numpy array
            if isinstance(points, list) and len(points) > 0:
                # Convert list to numpy array if needed
                points = np.array(points)
                ax.scatter(points[:, 0], points[:, 1], 
                            color=sampled_point_color,
                            s=sampled_point_size, 
                            marker=sampled_point_marker, 
                            edgecolors='white',
                            alpha=0.8,
                            zorder=10)  # Higher zorder to appear on top
            elif hasattr(points, 'size') and points.size > 0:
                ax.scatter(points[:, 0], points[:, 1], 
                            color=sampled_point_color,
                            s=sampled_point_size, 
                            marker=sampled_point_marker, 
                            edgecolors='white',
                            alpha=0.8,
                            zorder=10)
        logger.info(f"Sampled point visualization completed in {time.time() - sampled_points_start:.2f} seconds")
    
    ax.imshow(img)
    logger.info(f"Visualization completed in {time.time() - start_time:.2f} seconds")


def get_masks(
    model_name: str,
    image: Image.Image,
) -> List[Dict[str, Any]]:
    """
    Generate segmentation masks from an image using the specified model.
    
    This function uses a mask generator model (SAM or SAM2) to generate
    segmentation masks for an image. The results are cached for performance.
    
    Args:
        model_name: Name of the model to use ('sam', 'sam2', or 'sam_remote')
        image: PIL Image to generate masks for
    
    Returns:
        List of dictionaries containing mask data
    """

    logger.info("Generating masks - this may take some time")
    mask_start = time.time()

    if model_name == "sam_remote":
        logger.info("Using remote SAM model")
        sam_client = SAMClient()
        masks = sam_client.get_masks_from_pil_image(image)
    else:
        sam_mask_generator = get_sam_mask_generator(model_name)
        masks = sam_mask_generator.generate(np.array(image.convert("RGB")))

    logger.info(f"Generated {len(masks)} masks in {time.time() - mask_start:.2f} seconds")
    return masks


def sample_points_from_masks(masks: List[Dict[str, Any]], 
                            points_per_mask: int = 5, 
                            seed: Optional[int] = None) -> List[List[Tuple[int, int]]]:
    """
    Randomly sample points from each segmentation mask.
    
    This function takes a list of mask dictionaries (from get_masks) and randomly
    samples points from each mask. The points are returned as (x, y) coordinates.
    
    Args:
        masks: List of mask dictionaries returned by get_masks()
        points_per_mask: Number of points to sample from each mask
        seed: Random seed for reproducibility
        
    Returns:
        List of lists, where each inner list contains sampled points
        as (x, y) coordinate tuples for a mask
    """
    if seed is not None:
        random.seed(seed)
        np.random.seed(seed)
        
    all_sampled_points = []
    
    for i, mask in enumerate(masks):
        # Get the segmentation mask
        segmentation = mask['segmentation']
        
        # Find all coordinates where the mask is True (1)
        y_coords, x_coords = np.where(segmentation)
        
        # Combine into (x, y) coordinate pairs
        valid_points = list(zip(x_coords, y_coords))
        
        # If no valid points found, continue to the next mask
        if not valid_points:
            all_sampled_points.append([])
            continue
            
        # Sample points (or take all if fewer than requested)
        if len(valid_points) <= points_per_mask:
            sampled_points = valid_points
        else:
            # Randomly sample points without replacement
            indices = np.random.choice(len(valid_points), points_per_mask, replace=False)
            sampled_points = [valid_points[i] for i in indices]
        
        all_sampled_points.append(sampled_points)
        
    return all_sampled_points


def get_sam_mask_generator(name: str) -> Any:
    """
    Create a mask generator using the specified model.
    
    This function loads the appropriate model (SAM or SAM2) and initializes
    the automatic mask generator with the model.
    
    Args:
        name: Model name ('sam' or 'sam2')
    
    Returns:
        A mask generator object (SamAutomaticMaskGenerator or SAM2AutomaticMaskGenerator)
    
    Raises:
        ValueError: If the model name is not recognized
    """
    
    device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
    logger.info(f"Using device: {device}")
    model_start = time.time()

    if name == "sam":
        from segment_anything import sam_model_registry, SamAutomaticMaskGenerator

        sam_checkpoint: str = CUR_DIR / "3p/sam_ckpts/sam_vit_b_01ec64.pth"
        model_type: str = "vit_b"

        # Initialize the SAM model using the registry
        sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
        sam.to(device=device)
        logger.info(f"Model loaded in {time.time() - model_start:.2f} seconds")
        
        # Mask generation
        mask_start = time.time()
        logger.info("Initializing mask generator")
        mask_generator = SamAutomaticMaskGenerator(
            sam,
            pred_iou_thresh=0.95,
        )
        logger.info(f"Model loaded in {time.time() - model_start:.2f} seconds")

    elif name == "sam2":
        from sam2.build_sam import build_sam2
        from sam2.automatic_mask_generator import SAM2AutomaticMaskGenerator

        sam2_checkpoint = CUR_DIR / "3p/sam2_ckpts/sam2.1_hiera_large.pt"
        model_cfg = "configs/sam2.1/sam2.1_hiera_l.yaml"

        sam2 = build_sam2(model_cfg, sam2_checkpoint, device=device, apply_postprocessing=False)
        logger.info(f"Model loaded in {time.time() - model_start:.2f} seconds")
        
        # Mask generation
        mask_start = time.time()
        logger.info("Initializing mask generator")
        mask_generator = SAM2AutomaticMaskGenerator(sam2)
        logger.info(f"Model loaded in {time.time() - model_start:.2f} seconds")
        

    else:
        raise ValueError(f"Unknown model name: {name}")
    

    return mask_generator



def get_3d_point_from_pixel(pixel_coord: Tuple[int, int], 
                           depth_image: Optional[np.ndarray] = None) -> Optional[np.ndarray]:
    """
    Convert a 2D pixel coordinate to a 3D point using the RealSense depth camera.
    
    This function uses the RealSense depth camera to convert a 2D pixel coordinate
    to a 3D point in the camera frame. It requires ROS to be running and the
    RealSense camera to be publishing depth images and camera info.
    
    Args:
        pixel_coord: (x, y) pixel coordinate
        depth_image: Optional pre-captured depth image (if None, a new one will be captured)
        
    Returns:
        3D point in camera frame (x, y, z) in meters, or None if conversion fails
    """
    # Extract pixel coordinates
    x, y = int(pixel_coord[0]), int(pixel_coord[1])
    
    # Make sure ROS node is initialized
    if not rospy.core.is_initialized():
        logger.info("Initializing ROS node for 3D point conversion")
        rospy.init_node('pixel_to_3d_converter', anonymous=True)
        logger.info("ROS node initialized")
    
    try:
        if depth_image is None:
            # Get depth image
            logger.info(f"Waiting for depth image on topic '/camera/aligned_depth_to_color/image_raw'")
            depth_msg = rospy.wait_for_message('/camera/aligned_depth_to_color/image_raw', ROSSensorImage, timeout=2.0)
            logger.info(f"Received depth image")
            
            # Convert ROS Image message to numpy array
            from cv_bridge import CvBridge
            bridge = CvBridge()
            depth_image = bridge.imgmsg_to_cv2(depth_msg, desired_encoding="passthrough")
        
        # Get camera intrinsics
        logger.info(f"Waiting for camera info on topic '/camera/aligned_depth_to_color/camera_info'")
        camera_info_msg = rospy.wait_for_message('/camera/aligned_depth_to_color/camera_info', CameraInfo, timeout=2.0)
        logger.info(f"Received camera info")
        
        # Create RealSense intrinsics object
        intrinsics = rs.intrinsics()
        intrinsics.width = camera_info_msg.width
        intrinsics.height = camera_info_msg.height
        intrinsics.ppx = camera_info_msg.K[2]  # Principal point x
        intrinsics.ppy = camera_info_msg.K[5]  # Principal point y
        intrinsics.fx = camera_info_msg.K[0]   # Focal length x
        intrinsics.fy = camera_info_msg.K[4]   # Focal length y
        
        # Set the distortion model (RealSense uses Brown-Conrady model)
        intrinsics.model = rs.distortion.brown_conrady
        intrinsics.coeffs = list(camera_info_msg.D[:5])  # RealSense uses 5 distortion coefficients
        
        # Get depth value at the specified pixel (in millimeters)
        depth_mm = depth_image[y, x]
        
        # Check if depth is valid
        if depth_mm <= 0 or np.isnan(depth_mm):
            logger.warning(f"Invalid depth value at pixel ({x}, {y}): {depth_mm}")
            return None
        
        # Convert depth to meters for deproject function
        depth_m = depth_mm / 1000.0
        
        # Deproject to 3D point using RealSense library
        point_3d = rs.rs2_deproject_pixel_to_point(intrinsics, [x, y], depth_m)
        
        return np.array(point_3d)
        
    except Exception as e:
        logger.error(f"Error getting 3D point: {e}")
        logger.error(traceback.format_exc())
        return None


def create_point_marker(point_3d: np.ndarray, 
                       point_idx: int, 
                       frame_id: str = "camera_color_optical_frame") -> Marker:
    """
    Create a visualization marker for a 3D point.
    
    This function creates a ROS visualization marker for a 3D point.
    The marker is a sphere at the specified position.
    
    Args:
        point_3d: 3D point coordinates [x, y, z]
        point_idx: Index or ID for the marker
        frame_id: The coordinate frame to use
        
    Returns:
        ROS visualization marker
    """
    marker = Marker()
    marker.header.frame_id = frame_id
    marker.header.stamp = rospy.Time.now()
    marker.ns = "sampled_points"
    marker.id = point_idx
    marker.type = Marker.SPHERE
    marker.action = Marker.ADD
    
    # Set position
    marker.pose.position.x = point_3d[0]
    marker.pose.position.y = point_3d[1]
    marker.pose.position.z = point_3d[2]
    
    # Set orientation (identity quaternion)
    marker.pose.orientation.x = 0.0
    marker.pose.orientation.y = 0.0
    marker.pose.orientation.z = 0.0
    marker.pose.orientation.w = 1.0
    
    # Set scale
    marker.scale.x = 0.02  # 2cm sphere
    marker.scale.y = 0.02
    marker.scale.z = 0.02
    
    # Set color (lime green, matching your 2D visualization)
    marker.color.r = 0.0
    marker.color.g = 1.0
    marker.color.b = 0.0
    marker.color.a = 1.0
    
    # Set lifetime (0 = forever)
    marker.lifetime = rospy.Duration(0)
    
    return marker


def create_text_marker(point_3d: np.ndarray, 
                      point_idx: int, 
                      text: str, 
                      frame_id: str = "camera_color_optical_frame") -> Marker:
    """
    Create a text marker to label a 3D point.
    
    This function creates a ROS visualization marker with text to label a 3D point.
    The text is positioned slightly above the point.
    
    Args:
        point_3d: 3D point coordinates [x, y, z]
        point_idx: Index or ID for the marker
        text: Text to display
        frame_id: The coordinate frame to use
        
    Returns:
        ROS visualization marker
    """
    marker = Marker()
    marker.header.frame_id = frame_id
    marker.header.stamp = rospy.Time.now()
    marker.ns = "point_labels"
    marker.id = point_idx
    marker.type = Marker.TEXT_VIEW_FACING
    marker.action = Marker.ADD
    
    # Set position (slightly above the point)
    marker.pose.position.x = point_3d[0]
    marker.pose.position.y = point_3d[1]
    marker.pose.position.z = point_3d[2] + 0.03  # 3cm above the point
    
    # Set orientation (identity quaternion)
    marker.pose.orientation.x = 0.0
    marker.pose.orientation.y = 0.0
    marker.pose.orientation.z = 0.0
    marker.pose.orientation.w = 1.0
    
    # Set scale (text height in meters)
    marker.scale.z = 0.02
    
    # Set color (white text)
    marker.color.r = 1.0
    marker.color.g = 1.0
    marker.color.b = 1.0
    marker.color.a = 1.0
    
    # Set text
    marker.text = text
    
    # Set lifetime (0 = forever)
    marker.lifetime = rospy.Duration(0)
    
    return marker


def capture_live_image(bridge: Optional[CvBridge] = None) -> Optional[Image.Image]:
    """
    Capture a live image from the RealSense camera.
    
    This function captures a live RGB image from the RealSense camera
    using ROS.
    
    Args:
        bridge: CvBridge for converting ROS images (created if None)
        
    Returns:
        Captured PIL image from the camera, or None if capture fails
    """
    if bridge is None:
        bridge = CvBridge()
    
    logger.info("Waiting for color image from camera...")
    try:
        # Wait for a color image from the RealSense camera
        color_msg = rospy.wait_for_message('/camera/color/image_raw', ROSSensorImage, timeout=5.0)
        logger.info("Color image received successfully")
        
        # Convert ROS image to OpenCV format
        cv_image = bridge.imgmsg_to_cv2(color_msg, desired_encoding="rgb8")
        
        # Convert OpenCV image to PIL Image
        pil_image = Image.fromarray(cv_image)
        
        return pil_image
    except Exception as e:
        logger.error(f"Failed to capture live image: {e}")
        logger.error(traceback.format_exc())
        return None



def transform_point(point: Tuple[float, float, float], 
                   source_frame: str, 
                   target_frame: str) -> np.ndarray:
    """
    Transform point from one coordinate frame to another.
    
    This function uses ROS TF2 to transform a 3D point from one
    coordinate frame to another.
    
    Args:
        point: 3D point coordinates (x, y, z)
        source_frame: Source coordinate frame
        target_frame: Target coordinate frame
    
    Returns:
        Transformed point as numpy array [x, y, z]
    
    Raises:
        RobotOperationException: If the transform fails
    """
    logger.info(f"Transforming point {point} from {source_frame} to {target_frame}")
    
    # Transform point from camera frame to robot base frame
    try:        
        # Create a TF buffer and listener
        tf_buffer = tf2_ros.Buffer()
        tf_listener = tf2_ros.TransformListener(tf_buffer)
        
        # Wait for the transform to be available
        logger.info(f"Waiting for transform from {source_frame} to {target_frame}")
        sleep(1.0)  # Give time for the TF system to initialize
        
        # Create a PointStamped message for the camera point
        point_stamped = geometry_msgs.msg.PointStamped()
        point_stamped.header.frame_id = source_frame
        point_stamped.header.stamp = rospy.Time(0)
        point_stamped.point.x = point[0]
        point_stamped.point.y = point[1]
        point_stamped.point.z = point[2]
        
        # Transform the point to the robot base frame
        transformed_point = tf_buffer.transform(point_stamped, target_frame)
        
        # Extract the transformed coordinates
        x = transformed_point.point.x
        y = transformed_point.point.y
        z = transformed_point.point.z
        
        logger.info(f"Transformed point: ({x}, {y}, {z}) in {target_frame} frame")
        
    except (tf2_ros.LookupException, tf2_ros.ConnectivityException, tf2_ros.ExtrapolationException) as e:
        logger.error(f"Transform error: {e}")
        logger.error(traceback.format_exc())
        raise RobotOperationException(f"Transform error: {e}")
    
    # Return the transformed point
    return np.array([x, y, z])



def set_gripper_orientation(bot: InterbotixManipulatorXS, 
                           roll: float = 0, 
                           pitch: float = 0, 
                           yaw: float = 0) -> None:
    """
    Set the gripper orientation using roll, pitch, and yaw angles.
    
    This function sets the orientation of the robot gripper using
    Euler angles.
    
    Args:
        bot: InterbotixManipulatorXS object
        roll: Roll angle in radians
        pitch: Pitch angle in radians
        yaw: Yaw angle in radians
    """
    logger.info(f"Setting gripper orientation to roll={roll}, pitch={pitch}, yaw={yaw}")
    
    # Set end-effector pose with specified orientation
    bot.arm.set_ee_pose_components(
        roll=roll, pitch=pitch, yaw=yaw
    )
    sleep(1.0)  # Give time for the move to complete



### START OF DSL ###

def query_vlm(image: Image.Image, prompt: str) -> str:
    """
    Query a Vision Language Model (VLM) with a PIL image and prompt.
    This function sends an image and text prompt to GPT-4o and returns
    the model's response.
    
    Args:
        image: PIL Image object
        prompt: Text prompt to send with the image
    Returns:
        The VLM's text response
    """
    client = openai.OpenAI()
    
    # Determine image format
    img_format = image.format or "PNG"  # Default to PNG if format not specified
    mime = f"image/{img_format.lower()}"
    
    # Convert PIL Image to bytes
    buffer = io.BytesIO()
    image.save(buffer, format=img_format)
    buffer.seek(0)
    b64 = base64.b64encode(buffer.read()).decode()
    
    image_data_url = f"data:{mime};base64,{b64}"
    
    response = client.chat.completions.create(
        model="gpt-4o",
        messages=[
            {
                "role": "user",
                "content": [
                    {"type": "text",
                     "text": prompt},
                    {"type": "image_url",
                     "image_url": {
                         "url": image_data_url,
                         "detail": "high"
                     }}
                ]
            }
        ],
        max_tokens=250
    )
    
    logger.info(f"VLM response: {response}")
    return response.choices[0].message.content


def capture_scene_data(directory: Path = IMAGES_DIR) -> Tuple[Image.Image, np.ndarray]:
    """
    Capture image and depth data from the camera and save them.
    
    This function captures an RGB image and a depth image from the
    RealSense camera, saves them to disk, and returns them.
    
    Args:
        directory: Directory to save the captured images
    
    Returns:
        Tuple of (PIL Image, depth image as numpy array)
    
    Raises:
        ImageCaptureException: If image or depth data capture fails
    """
    # Create bridge for converting images
    bridge = CvBridge()
    
    # Generate timestamp for filenames
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    # Ensure directory exists
    directory.mkdir(parents=True, exist_ok=True)
    
    # Capture RGB image
    logger.info("Capturing live image from RealSense camera")
    image = capture_live_image(bridge)
    if image is None:
        raise ImageCaptureException("Failed to capture RGB image from camera")
    
    logger.info(f"Image captured successfully: {image.size}x{image.mode}")
    
    # Save RGB image
    image_path = directory / f"rgb_{timestamp}.png"
    image.save(image_path)
    logger.info(f"Image saved to: {image_path}")
    
    # Get depth image 
    try:
        logger.info("Waiting for depth image...")
        depth_msg = rospy.wait_for_message('/camera/aligned_depth_to_color/image_raw', ROSSensorImage, timeout=2.0)
        camera_info_msg = rospy.wait_for_message('/camera/aligned_depth_to_color/camera_info', CameraInfo, timeout=2.0)
    except rospy.ROSException as e:
        raise ImageCaptureException(f"Failed to receive depth image: {e}")
    
    # Convert ROS Image message to numpy array
    depth_image = bridge.imgmsg_to_cv2(depth_msg, desired_encoding="passthrough")
    
    # Optionally save depth image visualization
    plt.figure(figsize=(10, 10))
    plt.imshow(depth_image, cmap='viridis')
    plt.colorbar(label='Depth (mm)')
    depth_vis_path = directory / f"depth_{timestamp}.png"
    plt.savefig(depth_vis_path)
    plt.close()
    logger.info(f"Depth visualization saved to: {depth_vis_path}")
    
    return image, depth_image


def generate_keypoints(image: Image.Image, 
                      model_name: str = MODEL_NAME, 
                      seed: int = 42) -> Tuple[List[Dict[str, Any]], np.ndarray]:
    """
    Generate keypoints from mask generation.
    
    This function generates masks from an image using the specified model,
    then samples points from each mask to create keypoints.
    
    Args:
        image: Source image
        model_name: Name of the model to use
        seed: Random seed for point sampling
    
    Returns:
        Tuple of (masks, keypoints) where masks is a list of dictionaries
        and keypoints is a numpy array of (x, y) coordinates
    
    Raises:
        PointGenerationException: If mask generation or point sampling fails
    """
    # Generate masks
    logger.info(f"Generating masks using {model_name} model")
    try:
        masks = get_masks(model_name, image)
        logger.info(f"Successfully generated {len(masks)} masks")
    except Exception as e:
        logger.error(f"Error generating masks: {e}")
        logger.error(traceback.format_exc())
        raise PointGenerationException(f"Failed to generate masks: {e}")
    
    # Generate sampled points for each mask
    sampled_points_per_mask = sample_points_from_masks(masks, points_per_mask=2, seed=seed)
    
    # Flatten all points into a single list
    all_points = []
    for points in sampled_points_per_mask:
        all_points.extend(points)
    
    # Convert to numpy array if not empty
    if all_points:
        all_points = np.array(all_points)
    else:
        raise PointGenerationException("No points were sampled from masks")
    
    return masks, all_points


SELECT_KP_PROMPT = "Select a keypoint from the image with the following purpose: {purpose}. Make sure to select a point that will be inside the object with required. Only return the point label, no other text."


def select_keypoint(image: Image.Image, 
                   masks: List[Dict[str, Any]], 
                   points: np.ndarray, 
                   prompt: str) -> np.ndarray:
    """
    Visualize keypoints on image and select one point.
    
    This function creates a visualization of the image with keypoints,
    saves it to a file, and then selects one keypoint either via
    human input or a Vision Language Model.
    
    Args:
        image: Source image
        masks: Segmentation masks
        points: Array of keypoints as (x,y) coordinates
        prompt: Prompt for VLM selection
        
    Returns:
        Selected (x,y) point as a numpy array
    """
    # Generate timestamp for filename
    timestamp = time.strftime("%Y%m%d_%H%M%S")
    
    plt.figure(figsize=(20, 20))
    plt.imshow(image)
    
    # Show the annotations
    show_anns(
        masks,
        borders=False,
        show_boxes=False,
        show_points=False,
        show_masks=False,
        sampled_points=points,
        sampled_point_marker='o',
        sampled_point_size=50,
        sampled_point_color='lime'
    )
    
    # Add labels to each keypoint
    if len(points) > 0:
        ax = plt.gca()
        for i, (x, y) in enumerate(points):
            # Add text label with small offset from the point
            ax.annotate(f"p{i}", 
                       (x+5, y+5),
                       color='white',
                       fontsize=12,
                       fontweight='bold',
                       bbox=dict(facecolor='black', alpha=0.7, pad=2, edgecolor='none'))
    
    plt.axis('off')
    
    # Save the visualization to a file
    visualization_path = IMAGES_DIR / f"viz_{timestamp}.png"
    plt.savefig(visualization_path)
    plt.close()  # Close the figure to free memory
    
    # Print points information for selection
    print("\nAvailable points:")
    for i, (x, y) in enumerate(points):
        print(f"Point {i}: ({x}, {y})")
    
    print(f"\nVisualization saved to: {visualization_path}")
    
    # Handle point selection
    if not USE_VLM:
        # Human selection mode
        chosen_point_idx = int(input("\nEnter the index of the point you want to choose: ")[1:])
    else:
        # VLM selection mode
        chosen_point_response = query_vlm(
            image=Image.open(visualization_path),
            prompt=SELECT_KP_PROMPT.format(purpose=prompt),
        )
        print(f"\nVLM response: {chosen_point_response}")
        chosen_point_idx = int(chosen_point_response[1:])
    
    chosen_point = points[chosen_point_idx]
    print(f"Chosen point: {chosen_point}")
    return chosen_point

def project_to_3d(pixel_point: Tuple[int, int], 
                 depth_image: np.ndarray) -> np.ndarray:
    """
    Get 3D point from pixel coordinate and transform it.
    
    This function converts a 2D pixel coordinate to a 3D point using
    the depth image, then transforms it from the camera frame to the
    robot frame.
    
    Args:
        pixel_point: (x, y) pixel coordinate
        depth_image: Depth image as numpy array
    
    Returns:
        3D point in robot frame as numpy array [x, y, z]
    
    Raises:
        Point3DException: If 3D point conversion fails
    """
    logger.info(f"Getting 3D point for pixel: {pixel_point}")
    p3d = get_3d_point_from_pixel(pixel_point, depth_image)
    if p3d is None:
        raise Point3DException("Could not get 3D point. Check depth data.")
    
    logger.info(f"3D point in {CAMERA_FRAME}: {p3d}")

    # transform the point to the robot base frame
    p3d = transform_point(p3d, CAMERA_FRAME, ROBOT_FRAME)

    return p3d


def sleep(seconds: float) -> None:
    """
    Pause execution for the specified time with logging.
    
    This function is a wrapper around rospy.sleep that adds logging.
    
    Args:
        seconds: Time to sleep in seconds
    """
    logger.info(f"Sleeping for {seconds} seconds")
    rospy.sleep(seconds)


def open_gripper(bot: InterbotixManipulatorXS) -> None:
    """
    Open the robot gripper.
    
    This function opens the robot gripper and waits for the operation
    to complete.
    
    Args:
        bot: InterbotixManipulatorXS object
    """
    logger.info("Opening gripper")
    bot.gripper.open()
    sleep(1.0)


def close_gripper(bot: InterbotixManipulatorXS) -> None:
    """
    Close the robot gripper.
    
    This function closes the robot gripper and waits for the operation
    to complete.
    
    Args:
        bot: InterbotixManipulatorXS object
    """
    logger.info("Closing gripper")
    bot.gripper.close()
    sleep(1.0)


def go_home(bot: InterbotixManipulatorXS) -> None:
    """
    Move the robot to home position.
    
    This function moves the robot arm to its home position and waits
    for the operation to complete.
    
    Args:
        bot: InterbotixManipulatorXS object
    """
    logger.info("Moving arm to home position")
    bot.arm.go_to_home_pose()
    sleep(1.0)


def move_to_point(bot: InterbotixManipulatorXS, 
                 x: float, 
                 y: float, 
                 z: float) -> None:
    """
    Move the robot arm to a 3D point in the robot frame
    
    Args:
        bot: InterbotixManipulatorXS object
        x: X coordinate in robot frame
        y: Y coordinate in robot frame
        z: Z coordinate in robot frame
        
    Raises:
        RobotOperationException: If the movement fails
    """
    x, y, z = transform_point((x, y, z), ROBOT_FRAME, CAMERA_FRAME)
    
    try:
        logger.info(f"Moving to position: ({x}, {y}, {z})")
        bot.arm.set_ee_pose_components(
            x=x, y=y, z=z + 0.01,
        )
        sleep(2.0)  # Give time for the move to complete

    except Exception as e:
        logger.error(f"Error during arm movement: {e}")
        logger.error(traceback.format_exc())
        # Try to recover by going to home position
        go_home(bot)
        raise RobotOperationException(f"Error during arm movement: {e}")


def move_by_offset(bot: InterbotixManipulatorXS, 
                  dx: float = 0, 
                  dy: float = 0, 
                  dz: float = 0) -> None:
    """
    Move the robot arm by a specified offset.
    
    This function moves the robot arm by the specified offset from
    its current position.
    
    Args:
        bot: InterbotixManipulatorXS object
        dx: Offset in x direction (meters)
        dy: Offset in y direction (meters)
        dz: Offset in z direction (meters)
    """
    logger.info(f"Moving by offset: dx={dx}, dy={dy}, dz={dz}")
    
    # Move the end-effector by the specified offset
    bot.arm.set_ee_cartesian_trajectory(x=dx, y=dy, z=dz)
    sleep(1.0)  # Give time for the move to complete


def rotate_by_offset(bot: InterbotixManipulatorXS, 
                    roll: float = 0, 
                    pitch: float = 0, 
                    yaw: float = 0) -> None:
    """
    Rotate the gripper by a specified offset.
    
    This function rotates the robot gripper by the specified Euler angle
    offsets from its current orientation.
    
    Args:
        bot: InterbotixManipulatorXS object
        roll: Roll angle offset in radians
        pitch: Pitch angle offset in radians
        yaw: Yaw angle offset in radians
    """
    logger.info(f"Rotating by offset: roll={roll}, pitch={pitch}, yaw={yaw}")
    
    # Rotate the end-effector by the specified angles
    bot.arm.set_ee_pose_components(
        roll=roll, pitch=pitch, yaw=yaw
    )
    sleep(1.0)  # Give time for the move to complete


def set_gripper_pose(bot: InterbotixManipulatorXS, 
                    x: float, 
                    y: float, 
                    z: float, 
                    roll: float = 0, 
                    pitch: float = 0, 
                    yaw: float = 0) -> None:
    """
    Set the gripper pose using position and orientation.
    
    This function sets the position and orientation of the robot gripper
    in the robot frame.
    
    Args:
        bot: InterbotixManipulatorXS object
        x: X coordinate in robot frame
        y: Y coordinate in robot frame
        z: Z coordinate in robot frame
        roll: Roll angle in radians
        pitch: Pitch angle in radians
        yaw: Yaw angle in radians
    """
    logger.info(f"Setting gripper pose to x={x}, y={y}, z={z}, roll={roll}, pitch={pitch}, yaw={yaw}")
    
    # Set end-effector pose with specified position and orientation
    bot.arm.set_ee_pose_components(
        x=x, y=y, z=z,
        roll=roll, pitch=pitch, yaw=yaw
    )
    sleep(2.0)  # Give time for the move to complete



### END OF DSL ###

SELECTION_PROMPT = """Which point should I grasp to pick up the green object? I prefer the point on the body of the object. Only return the point, no other text."""

# SELECTION_PROMPT = """Which point should I grasp to pick up the the more rigid object? I prefer the point on the body of the object. Only return the point, no other text."""



def control_robot(bot):
    image, depth_image = capture_scene_data()
    
    masks, keypoints = generate_keypoints(image)
    chosen_keypoint = select_keypoint(image, masks, keypoints, SELECTION_PROMPT)
    p3d = project_to_3d(chosen_keypoint, depth_image)
    
    
    # First move to a position slightly above the target
    approach_z_offset = 0.1  # Increased for more clearance when approaching vertically

    x, y, z = p3d
    
    # Set end-effector pose with downward-pointing orientation
    # Roll = 0, Pitch = 1.5708 (90 degrees), Yaw = 0
    # This makes the gripper point downward (along negative Z-axis)
    set_gripper_pose(bot, x, y, z + approach_z_offset, roll=0, pitch=1.5708, yaw=0)
    
    # Move down to the target position
    move_by_offset(bot, dz=-approach_z_offset)
    
    # Close the gripper to grasp the object
    close_gripper(bot)
    
    # Lift the object
    move_by_offset(bot, dz=approach_z_offset * 2)


def control_robot(bot):
    image, depth_image = capture_scene_data()                                                                                    
    masks, keypoints = generate_keypoints(image)                                                                                 
    green_alien_point = select_keypoint(image, masks, keypoints, "green alien figure")                                                  
    position_3d = project_to_3d(green_alien_point, depth_image)                                                                  
                                                                                                                                    
    x, y, z = position_3d                                                                                                        
    z += 0.1  # Move slightly above the ground                                                                                      
    set_gripper_pose(bot, x, y, z, roll=0, pitch=1.5708, yaw=0)  # Point gripper downwards                                                                                                                         
    z -= 0.12  # Move down to the ground level                                                                                    
    set_gripper_pose(bot, x, y, z, roll=0, pitch=1.5708, yaw=0)  # Adjust gripper position                                       
    close_gripper(bot)   
    move_by_offset(bot, dz=0.1)  # Lift the object                              
    go_home(bot)
    open_gripper(bot)  # Release the object


def main():
    """Main function orchestrating the entire process."""

    # Initialize ROS node if not already initialized
    if not rospy.core.is_initialized():
        rospy.init_node('steepmind', anonymous=True)
        logger.info("ROS node 'steepmind' initialized")

    bot = InterbotixManipulatorXS("wx250s", "arm", "gripper", init_node=False)
    bot.arm.go_to_sleep_pose()
    bot.gripper.open()

    try:
        control_robot(bot)
        pass
        
    except Exception as e:
        logger.error(f"Attempt failed: {e}")
        logger.error(traceback.format_exc())
    
    bot.gripper.open()
    bot.arm.go_to_sleep_pose()

if __name__ == '__main__':
    import fire
    fire.Fire(main)